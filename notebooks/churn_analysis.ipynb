{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gestion de la fidélité client – Analyse Churn Télécom\n",
        "\n",
        "Notebook conçu pour un entretien de consultant data : il présente une démarche structurée pour analyser le churn client, construire un modèle prédictif et proposer des recommandations actionnables."
      ],
      "id": "be65ebc2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectifs du notebook\n",
        "- Comprendre les facteurs expliquant la résiliation des clients (churn).\n",
        "- Construire des modèles simples et interprétables pour prédire le churn.\n",
        "- Préparer un support d'oral clair, avec commentaires pédagogiques."
      ],
      "id": "3a5d8ba4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plan du notebook\n",
        "1. Imports & chargement des données\n",
        "2. Analyse exploratoire rapide orientée churn\n",
        "3. Préparation des données & feature engineering\n",
        "4. Modélisation & comparaison de modèles\n",
        "5. Interprétabilité & importance des variables\n",
        "6. Conclusion & recommandations métier\n",
        "7. Fonction de scoring `predict()`\n"
      ],
      "id": "plan_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "⚙️ **Préparation de l'environnement**\\n",
        "Si vous exécutez ce notebook sur une instance fraîche (ex. Google Colab), lancez la cellule suivante une fois pour installer les dépendances externes :\\n",
        "```python\n!pip install shap xgboost\n```\n"
      ],
      "id": "setupnote"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Imports & configuration générale\n",
        "# Objectif : charger les librairies nécessaires à l'EDA, la modélisation et l'interprétabilité.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "np.random.seed(42)"
      ],
      "id": "5e2b8d72"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Chargement des données\n",
        "Nous travaillons sur l'échantillon mis à disposition par l'opérateur Télécom. L'objectif est de partir du fichier brut, de contrôler sa qualité et de prendre en main les variables clés."
      ],
      "id": "5687376a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Chargement des données\n",
        "# On lit directement le fichier CSV via son URL (simulation d'un accès data hébergé).\n",
        "\n",
        "DATA_URL = \"https://raw.githubusercontent.com/YanisBoNueve/wavestone_test/refs/heads/main/data/sujet_B_data_client_churn.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_URL)\n",
        "print(f\"Dimensions du dataset : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "df.head()"
      ],
      "id": "91f3e05e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Inspection du schéma de données\n",
        "# L'objectif est de comprendre les types de variables et d'identifier d'éventuelles incohérences.\n",
        "\n",
        "df.info()"
      ],
      "id": "f1ccddae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Profil des valeurs manquantes\n",
        "# Nous quantifions les données manquantes pour anticiper les traitements d'imputation.\n",
        "\n",
        "missing_profile = (\n",
        "    df.isna()\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .rename(columns={\"index\": \"variable\", 0: \"nb_valeurs_manquantes\"})\n",
        ")\n",
        "missing_profile[\"taux_manquant\"] = missing_profile[\"nb_valeurs_manquantes\"] / len(df)\n",
        "missing_profile.sort_values(\"taux_manquant\", ascending=False)"
      ],
      "id": "6f99ca66"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Analyse exploratoire orientée churn\n",
        "Approche pragmatique : comprendre comment se répartissent les résiliations et quelles variables semblent discriminantes. On se concentre sur les variables socio-contractuelles fréquemment utilisées dans les cas Télécom."
      ],
      "id": "6728bf18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Préparation de variables d'analyse\n",
        "# Création d'indicateurs utiles : taux de churn numérique et nombre de services souscrits.\n",
        "\n",
        "# Cible numérique pour les calculs de taux\n",
        "churn_flag = df[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# Liste des services considérés\n",
        "service_categories = {\n",
        "    \"PhoneService\": [\"Yes\"],\n",
        "    \"MultipleLines\": [\"Yes\"],\n",
        "    \"InternetService\": [\"DSL\", \"Fiber optic\"],\n",
        "    \"OnlineSecurity\": [\"Yes\"],\n",
        "    \"OnlineBackup\": [\"Yes\"],\n",
        "    \"DeviceProtection\": [\"Yes\"],\n",
        "    \"TechSupport\": [\"Yes\"],\n",
        "    \"StreamingTV\": [\"Yes\"],\n",
        "    \"StreamingMovies\": [\"Yes\"],\n",
        "}\n",
        "\n",
        "service_counts = pd.DataFrame({\n",
        "    col: df[col].isin(valid_values).astype(int)\n",
        "    for col, valid_values in service_categories.items()\n",
        "})\n",
        "\n",
        "df[\"NumServices\"] = service_counts.sum(axis=1)\n",
        "print(\"Distribution du nombre de services souscrits :\")\n",
        "df[\"NumServices\"].value_counts().sort_index()"
      ],
      "id": "9359020c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Taux global de churn\n",
        "# Première photographie : quelle proportion de clients résilient ?\n",
        "\n",
        "taux_churn = churn_flag.mean()\n",
        "print(f\"Taux global de churn : {taux_churn:.2%}\")\n",
        "\n",
        "sns.countplot(data=df, x=\"Churn\", palette=\"Blues\")\n",
        "plt.title(\"Répartition du churn\")\n",
        "plt.show()"
      ],
      "id": "5d82c24f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Fonction utilitaire pour analyser le churn par variable\n",
        "\n",
        "def churn_rate_by_variable(data, column, bins=None, order=None):\n",
        "    temp = data.copy()\n",
        "    temp[\"ChurnFlag\"] = temp[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "    group_col = column\n",
        "    if bins is not None:\n",
        "        group_col = f\"{column}_bin\"\n",
        "        temp[group_col] = pd.cut(temp[column], bins=bins, include_lowest=True)\n",
        "\n",
        "    summary = (\n",
        "        temp.groupby(group_col)[\"ChurnFlag\"]\n",
        "            .agg([\"count\", \"mean\"])\n",
        "            .rename(columns={\"count\": \"nb_clients\", \"mean\": \"taux_churn\"})\n",
        "            .reset_index()\n",
        "    )\n",
        "    if order is not None:\n",
        "        summary[group_col] = pd.Categorical(summary[group_col], categories=order, ordered=True)\n",
        "        summary = summary.sort_values(group_col)\n",
        "\n",
        "    return summary"
      ],
      "id": "bfdffe47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Analyse churn vs Tenure (ancienneté)\n",
        "\n",
        "tenure_bins = [0, 12, 24, 36, 48, 60, 72]\n",
        "summary_tenure = churn_rate_by_variable(df, \"tenure\", bins=tenure_bins)\n",
        "display(summary_tenure)\n",
        "\n",
        "sns.barplot(data=summary_tenure, x=\"tenure_bin\", y=\"taux_churn\", palette=\"viridis\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Taux de churn par ancienneté (Tenure)\")\n",
        "plt.ylabel(\"Taux de churn\")\n",
        "plt.xlabel(\"Ancienneté (mois)\")\n",
        "plt.show()"
      ],
      "id": "fea9ea57"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Analyse churn vs Contract\n",
        "summary_contract = churn_rate_by_variable(df, \"Contract\")\n",
        "display(summary_contract)\n",
        "\n",
        "sns.barplot(data=summary_contract, x=\"Contract\", y=\"taux_churn\", palette=\"viridis\")\n",
        "plt.title(\"Taux de churn par type de contrat\")\n",
        "plt.ylabel(\"Taux de churn\")\n",
        "plt.xlabel(\"Type de contrat\")\n",
        "plt.show()"
      ],
      "id": "3d87dd07"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Analyse churn vs PaymentMethod\n",
        "summary_payment = churn_rate_by_variable(df, \"PaymentMethod\")\n",
        "display(summary_payment)\n",
        "\n",
        "sns.barplot(data=summary_payment, x=\"PaymentMethod\", y=\"taux_churn\", palette=\"viridis\")\n",
        "plt.title(\"Taux de churn par méthode de paiement\")\n",
        "plt.ylabel(\"Taux de churn\")\n",
        "plt.xlabel(\"Méthode de paiement\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.show()"
      ],
      "id": "af7d3d03"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Analyse churn vs MonthlyCharges\n",
        "charges_bins = [0, 30, 60, 90, 120]\n",
        "summary_monthly = churn_rate_by_variable(df, \"MonthlyCharges\", bins=charges_bins)\n",
        "display(summary_monthly)\n",
        "\n",
        "sns.barplot(data=summary_monthly, x=\"MonthlyCharges_bin\", y=\"taux_churn\", palette=\"viridis\")\n",
        "plt.title(\"Taux de churn par niveau de facturation mensuelle\")\n",
        "plt.ylabel(\"Taux de churn\")\n",
        "plt.xlabel(\"Monthly Charges\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "id": "ccb568d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Analyse churn vs PaperlessBilling\n",
        "summary_paperless = churn_rate_by_variable(df, \"PaperlessBilling\")\n",
        "display(summary_paperless)\n",
        "\n",
        "sns.barplot(data=summary_paperless, x=\"PaperlessBilling\", y=\"taux_churn\", palette=\"viridis\")\n",
        "plt.title(\"Taux de churn selon la facturation dématérialisée\")\n",
        "plt.ylabel(\"Taux de churn\")\n",
        "plt.xlabel(\"Paperless Billing\")\n",
        "plt.show()"
      ],
      "id": "6b9e5ec7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Analyse churn vs InternetService\n",
        "summary_internet = churn_rate_by_variable(df, \"InternetService\")\n",
        "display(summary_internet)\n",
        "\n",
        "sns.barplot(data=summary_internet, x=\"InternetService\", y=\"taux_churn\", palette=\"viridis\")\n",
        "plt.title(\"Taux de churn selon le type d'accès internet\")\n",
        "plt.ylabel(\"Taux de churn\")\n",
        "plt.xlabel(\"Internet Service\")\n",
        "plt.show()"
      ],
      "id": "637a345a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Relation churn vs nombre de services souscrits\n",
        "summary_services = churn_rate_by_variable(df, \"NumServices\")\n",
        "display(summary_services)\n",
        "\n",
        "sns.barplot(data=summary_services, x=\"NumServices\", y=\"taux_churn\", palette=\"viridis\")\n",
        "plt.title(\"Taux de churn en fonction du nombre de services\")\n",
        "plt.ylabel(\"Taux de churn\")\n",
        "plt.xlabel(\"Nombre de services actifs\")\n",
        "plt.show()"
      ],
      "id": "7490b8da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Préparation des données & feature engineering\n",
        "On prépare maintenant les données pour la modélisation : nettoyage de variables numériques, sélection des features pertinentes et création des objets de prétraitement."
      ],
      "id": "c00656bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Copie de travail pour la modélisation\n",
        "# Objectif : conserver le dataset brut tout en préparant un jeu propre.\n",
        "\n",
        "df_model = df.copy()\n",
        "\n",
        "# Conversion de TotalCharges en numérique (certaines valeurs peuvent être vides au départ)\n",
        "df_model[\"TotalCharges\"] = pd.to_numeric(df_model[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "# Vérification du taux de valeurs manquantes après conversion\n",
        "missing_after_conversion = df_model[\"TotalCharges\"].isna().mean()\n",
        "print(f\"Taux de valeurs manquantes sur TotalCharges après conversion : {missing_after_conversion:.2%}\")\n",
        "\n",
        "# Séparation features / cible\n",
        "if \"customerID\" in df_model.columns:\n",
        "    df_model = df_model.drop(columns=[\"customerID\"])\n",
        "\n",
        "X = df_model.drop(columns=[\"Churn\"])\n",
        "y = df_model[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "print(f\"Nombre de features disponibles pour la modélisation : {X.shape[1]}\")"
      ],
      "id": "beca7d8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Identification des types de variables\n",
        "# Cela permet de construire un pipeline d'encodage adapté.\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
        "\n",
        "print(\"Variables numériques :\", numeric_features)\n",
        "print(\"Variables catégorielles :\", categorical_features)"
      ],
      "id": "6810bc2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Préprocesseurs pour pipeline de modélisation\n",
        "# Imputation simple + encodage One-Hot pour les catégorielles.\n",
        "\n",
        "numeric_transformer = SimpleImputer(strategy=\"median\")\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")"
      ],
      "id": "991e743c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Découpage train/test\n",
        "# Un split simple (70/30) permet de se positionner rapidement sur la performance des modèles.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Taille du train : {X_train.shape[0]} lignes\")\n",
        "print(f\"Taille du test : {X_test.shape[0]} lignes\")"
      ],
      "id": "a656a580"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Modélisation & comparaison\n",
        "Nous comparons trois modèles complémentaires : régression logistique (baseline linéaire explicable), forêt aléatoire (non-linéaire robuste) et XGBoost (boosting gradient). L'objectif est de choisir un compromis performance / interprétabilité."
      ],
      "id": "d1ba41d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Entraînement des modèles\n",
        "# Chaque modèle est intégré dans un pipeline partageant le même préprocesseur.\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=300, max_depth=None, random_state=42, class_weight=\"balanced\"),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=400,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        use_label_encoder=False\n",
        "    ),\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "metrics = []\n",
        "confusion_matrices = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    metrics.append({\"Modèle\": name, \"Accuracy\": acc, \"F1\": f1})\n",
        "    confusion_matrices[name] = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    trained_models[name] = pipe\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics).sort_values(by=\"F1\", ascending=False)\n",
        "metrics_df"
      ],
      "id": "4590f95a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Visualisation de la matrice de confusion du meilleur modèle\n",
        "\n",
        "best_model_name = metrics_df.iloc[0][\"Modèle\"]\n",
        "best_pipeline = trained_models[best_model_name]\n",
        "best_conf_matrix = confusion_matrices[best_model_name]\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(best_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(f\"Matrice de confusion – {best_model_name}\")\n",
        "plt.xlabel(\"Prédictions\")\n",
        "plt.ylabel(\"Réalité\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Meilleur modèle sur F1-score : {best_model_name}\")"
      ],
      "id": "6370de01"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Interprétabilité des modèles\n",
        "Objectif : identifier les facteurs qui influencent le plus la prédiction. On s'appuie sur l'importance des variables des modèles arborescents et un aperçu SHAP."
      ],
      "id": "8b5e0994"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Importance des variables pour les modèles arborescents\n",
        "# Extraction des noms de features après prétraitement pour lecture métier.\n",
        "\n",
        "def get_feature_names(fitted_preprocessor):\n",
        "    return fitted_preprocessor.get_feature_names_out()\n",
        "\n",
        "feature_names = get_feature_names(best_pipeline.named_steps[\"preprocessor\"])\n",
        "\n",
        "importance_results = {}\n",
        "for name in [\"Random Forest\", \"XGBoost\"]:\n",
        "    if name in trained_models:\n",
        "        clf = trained_models[name].named_steps[\"classifier\"]\n",
        "        if hasattr(clf, \"feature_importances_\"):\n",
        "            importances = clf.feature_importances_\n",
        "            importance_df = (\n",
        "                pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "                  .sort_values(\"importance\", ascending=False)\n",
        "                  .head(15)\n",
        "            )\n",
        "            importance_results[name] = importance_df\n",
        "            display(f\"Top features – {name}\")\n",
        "            display(importance_df)\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            sns.barplot(data=importance_df, x=\"importance\", y=\"feature\", palette=\"mako\")\n",
        "            plt.title(f\"Importance des variables ({name})\")\n",
        "            plt.xlabel(\"Importance normalisée\")\n",
        "            plt.ylabel(\"Feature\")\n",
        "            plt.show()"
      ],
      "id": "a660b77e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Analyse SHAP pour le meilleur modèle arborescent\n",
        "# Permet de visualiser l'influence moyenne de chaque feature.\n",
        "\n",
        "shap_model_name = None\n",
        "for candidate in [\"XGBoost\", \"Random Forest\"]:\n",
        "    if candidate == best_model_name:\n",
        "        shap_model_name = candidate\n",
        "        break\n",
        "    if shap_model_name is None and candidate in trained_models:\n",
        "        shap_model_name = candidate\n",
        "\n",
        "if shap_model_name is not None:\n",
        "    tree_pipeline = trained_models[shap_model_name]\n",
        "    tree_clf = tree_pipeline.named_steps[\"classifier\"]\n",
        "    X_train_transformed = tree_pipeline.named_steps[\"preprocessor\"].transform(X_train)\n",
        "    if hasattr(X_train_transformed, \"toarray\"):\n",
        "        X_train_transformed = X_train_transformed.toarray()\n",
        "\n",
        "    feature_names_shap = tree_pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
        "\n",
        "    sample_size = min(1000, X_train_transformed.shape[0])\n",
        "    sample_indices = np.random.choice(X_train_transformed.shape[0], sample_size, replace=False)\n",
        "    X_sample = X_train_transformed[sample_indices]\n",
        "\n",
        "    explainer = shap.TreeExplainer(tree_clf)\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values_to_plot = shap_values[1]\n",
        "    else:\n",
        "        shap_values_to_plot = shap_values\n",
        "\n",
        "    shap.summary_plot(shap_values_to_plot, features=X_sample, feature_names=feature_names_shap, plot_type=\"bar\", show=False)\n",
        "    plt.title(f\"Importance SHAP – {shap_model_name}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Aucun modèle arborescent disponible pour SHAP.\")"
      ],
      "id": "25cb797e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion & recommandations\n",
        "- **Relation nombre de services / churn** : l'analyse montre que les clients multi-équipés résilient moins que ceux avec peu de services. L'hypothèse \"plus de services ↔ churn\" est globalement infirmée : développer l'upsell multi-services peut renforcer la fidélité.\n",
        "- **Facteurs clés** : l'ancienneté faible, les contrats sans engagement et la facturation électronique ressortent comme signaux de risque. Les charges mensuelles élevées augmentent légèrement le churn.\n",
        "- **Modèle retenu** : le meilleur modèle identifié (cf. cellule précédente – observé comme Random Forest sur l'échantillon) offre le meilleur équilibre F1 / interprétabilité. La forêt aléatoire fournit des importances lisibles et une bonne robustesse opérationnelle.\n\n",
        "### Prochaines étapes proposées\n",
        "1. **Validation & tuning léger** : cross-validation 5-fold, ajustement des hyperparamètres clés (profondeur forêt, régularisation XGBoost).\n",
        "2. **Amélioration qualité data** : suivi des valeurs manquantes `TotalCharges`, enrichissement avec des signaux d'usage réseau et historiques d'interactions SAV.\n",
        "3. **Industrialisation MLOps** : pipeline automatisé (feature store, modèle registry, suivi du drift et des performances), monitoring des scores en production.\n",
        "4. **Adoption métier** : guides d'usage pour les équipes fidélisation, alertes sur les clients à risque et intégration dans les parcours CRM.\n"
      ],
      "id": "47d090e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Fonction de scoring `predict()`\n",
        "La fonction ci-dessous illustre comment intégrer le modèle dans un produit data : on lui passe un dictionnaire représentant un client, et elle retourne la probabilité de churn et la classe prédite."
      ],
      "id": "354a7960"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- Fonction utilitaire de scoring\n",
        "# Elle permet de simuler l'utilisation du meilleur modèle dans un outil métier.\n",
        "\n",
        "feature_reference = X.columns.tolist()\n",
        "\n",
        "\n",
        "def predict(client_dict):\n",
        "    '''Retourne la probabilité de churn et la prédiction binaire pour un client.'''\n",
        "    client_df = pd.DataFrame([client_dict])\n",
        "\n",
        "    # Garantir que toutes les features attendues sont présentes\n",
        "    missing_features = set(feature_reference) - set(client_df.columns)\n",
        "    for feat in missing_features:\n",
        "        client_df[feat] = np.nan\n",
        "\n",
        "    client_df = client_df[feature_reference]\n",
        "\n",
        "    proba = best_pipeline.predict_proba(client_df)[0, 1]\n",
        "    prediction = \"Yes\" if proba >= 0.5 else \"No\"\n",
        "\n",
        "    return {\"churn_probability\": float(proba), \"churn_prediction\": prediction}\n",
        "\n",
        "\n",
        "# Exemple d'utilisation (les valeurs sont indicatives)\n",
        "example_client = {\n",
        "    \"gender\": \"Female\",\n",
        "    \"SeniorCitizen\": 0,\n",
        "    \"Partner\": \"Yes\",\n",
        "    \"Dependents\": \"No\",\n",
        "    \"tenure\": 5,\n",
        "    \"PhoneService\": \"Yes\",\n",
        "    \"MultipleLines\": \"No\",\n",
        "    \"InternetService\": \"Fiber optic\",\n",
        "    \"OnlineSecurity\": \"No\",\n",
        "    \"OnlineBackup\": \"No\",\n",
        "    \"DeviceProtection\": \"No\",\n",
        "    \"TechSupport\": \"No\",\n",
        "    \"StreamingTV\": \"Yes\",\n",
        "    \"StreamingMovies\": \"Yes\",\n",
        "    \"Contract\": \"Month-to-month\",\n",
        "    \"PaperlessBilling\": \"Yes\",\n",
        "    \"PaymentMethod\": \"Electronic check\",\n",
        "    \"MonthlyCharges\": 95.5,\n",
        "    \"TotalCharges\": 500.0,\n",
        "    \"NumServices\": 5\n",
        "}\n",
        "\n",
        "predict(example_client)"
      ],
      "id": "b1dba9a1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}